{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colabの環境整備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1363
    },
    "colab_type": "code",
    "id": "u1ypELgCkFu_",
    "outputId": "550f0a9e-b3b5-4d52-af00-de12fc245d40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt -y -q install cuda-libraries-dev-9-2\n",
    "!pip install -q cupy-cuda92 chainer\n",
    "!pip install -U -q PyDrive\n",
    "!pip install chainercv\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習する\n",
    "\n",
    "[chainercvのSSDのtrain.py](https://github.com/chainer/chainercv/blob/master/examples/ssd/train.py)を適宜変更したもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-Gj-sMPOQ-g"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from ss2dbbox_dataset import SS2DBboxDataset\n",
    "from ss2dbbox_dataset import ss2d_bbox_label_names\n",
    "\n",
    "import chainer\n",
    "from chainer.datasets import ConcatenatedDataset\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainer.optimizer_hooks import WeightDecay\n",
    "from chainer import serializers\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.training import triggers\n",
    "\n",
    "from detection_voc_evaluator_zero import DetectionVOCEvaluator\n",
    "from chainercv.links.model.ssd import GradientScaling\n",
    "\n",
    "\n",
    "from chainercv.links.model.ssd import multibox_loss\n",
    "from chainercv.links import SSD300\n",
    "from chainercv.links import SSD512\n",
    "from chainercv import transforms\n",
    "from chainercv import utils\n",
    "\n",
    "\n",
    "from chainercv.links.model.ssd import random_crop_with_bbox_constraints\n",
    "from chainercv.links.model.ssd import random_distort\n",
    "from chainercv.links.model.ssd import resize_with_random_interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiboxTrainChain(chainer.Chain):\n",
    "\n",
    "    def __init__(self, model, alpha=1, k=3):\n",
    "        super(MultiboxTrainChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, imgs, gt_mb_locs, gt_mb_labels):\n",
    "        mb_locs, mb_confs = self.model(imgs)\n",
    "        loc_loss, conf_loss = multibox_loss(\n",
    "            mb_locs, mb_confs, gt_mb_locs, gt_mb_labels, self.k)\n",
    "        loss = loc_loss * self.alpha + conf_loss\n",
    "\n",
    "        chainer.reporter.report(\n",
    "            {'loss': loss, 'loss/loc': loc_loss, 'loss/conf': conf_loss},\n",
    "            self)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Transform(object):\n",
    "\n",
    "    def __init__(self, coder, size, mean):\n",
    "        # to send cpu, make a copy\n",
    "        self.coder = copy.copy(coder)\n",
    "        self.coder.to_cpu()\n",
    "\n",
    "        self.size = size\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, in_data):\n",
    "        # There are five data augmentation steps\n",
    "        # 1. Color augmentation\n",
    "        # 2. Random expansion\n",
    "        # 3. Random cropping\n",
    "        # 4. Resizing with random interpolation\n",
    "        # 5. Random horizontal flipping\n",
    "\n",
    "        img, bbox, label, nazo= in_data\n",
    "\n",
    "        # 1. Color augmentation\n",
    "        img = random_distort(img)\n",
    "\n",
    "        # 2. Random expansion\n",
    "        if np.random.randint(2):\n",
    "            img, param = transforms.random_expand(\n",
    "                img, fill=self.mean, return_param=True)\n",
    "            if len(bbox)>0:\n",
    "                bbox = transforms.translate_bbox(\n",
    "                    bbox, y_offset=param['y_offset'], x_offset=param['x_offset'])\n",
    "            else:\n",
    "                bbox = bbox.copy()\n",
    "        # 3. Random cropping\n",
    "        img, param = random_crop_with_bbox_constraints(\n",
    "            img, bbox, return_param=True)\n",
    "        if len(bbox)>0:\n",
    "          bbox, param = transforms.crop_bbox(\n",
    "            bbox, y_slice=param['y_slice'], x_slice=param['x_slice'],\n",
    "            allow_outside_center=False, return_param=True)\n",
    "          label = label[param['index']]\n",
    "\n",
    "        # 4. Resizing with random interpolatation\n",
    "        _, H, W = img.shape\n",
    "        img = resize_with_random_interpolation(img, (self.size, self.size))\n",
    "        if len(bbox)>0:\n",
    "          bbox = transforms.resize_bbox(bbox, (H, W), (self.size, self.size))\n",
    "\n",
    "        # 5. Random horizontal flipping\n",
    "        img, params = transforms.random_flip(\n",
    "            img, x_random=True, return_param=True)\n",
    "        if len(bbox)>0:\n",
    "          bbox = transforms.flip_bbox(\n",
    "            bbox, (self.size, self.size), x_flip=params['x_flip'])\n",
    "\n",
    "        # Preparation for SSD network\n",
    "        img -= self.mean\n",
    "        mb_loc, mb_label = self.coder.encode(bbox, label)\n",
    "\n",
    "        return img, mb_loc, mb_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3703
    },
    "colab_type": "code",
    "id": "dXPx0sSQ5BQj",
    "outputId": "fe8020b8-b09f-45e7-8c54-167867f7c0e0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpu=0\n",
    "out='result'\n",
    "resume=False\n",
    "batchsize=16\n",
    "\n",
    "# モデル\n",
    "model = SSD300(\n",
    "    n_fg_class=1,\n",
    "    pretrained_model='imagenet')\n",
    "\n",
    "model.use_preset('evaluate')\n",
    "train_chain = MultiboxTrainChain(model)\n",
    "chainer.cuda.get_device_from_id(gpu).use()\n",
    "model.to_gpu()\n",
    "\n",
    "# データセット\n",
    "train = TransformDataset(\n",
    "        SS2DBboxDataset(data_dir=\"dataset\", split='train',use_difficult=True, return_difficult=True),\n",
    "    Transform(model.coder, model.insize, model.mean))\n",
    "train_iter = chainer.iterators.MultiprocessIterator(train, batchsize)\n",
    "\n",
    "test = SS2DBboxDataset(data_dir=\"dataset\", split='test',\n",
    "    use_difficult=True, return_difficult=True)\n",
    "test_iter = chainer.iterators.SerialIterator(\n",
    "    test, batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = chainer.optimizers.MomentumSGD()\n",
    "optimizer.setup(train_chain)\n",
    "for param in train_chain.params():\n",
    "    if param.name == 'b':\n",
    "        param.update_rule.add_hook(GradientScaling(2))\n",
    "    else:\n",
    "        param.update_rule.add_hook(WeightDecay(0.0005))\n",
    "\n",
    "# Updater\n",
    "updater = training.updaters.StandardUpdater(\n",
    "    train_iter, optimizer, device=gpu)\n",
    "\n",
    "# Trainerの設定\n",
    "trainer = training.Trainer(updater, (2000, 'iteration'), out)\n",
    "trainer.extend(\n",
    "    extensions.ExponentialShift('lr', 0.1, init=1e-3),\n",
    "    trigger=triggers.ManualScheduleTrigger([3000, 5000], 'iteration'))\n",
    "\n",
    "trainer.extend(\n",
    "    DetectionVOCEvaluator(\n",
    "        test_iter, model, use_07_metric=True,\n",
    "        label_names=ss2d_bbox_label_names),\n",
    "    trigger=(100, 'iteration'))\n",
    "\n",
    "log_interval = 10, 'iteration'\n",
    "trainer.extend(extensions.LogReport(trigger=log_interval))\n",
    "trainer.extend(extensions.observe_lr(), trigger=log_interval)\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'iteration', 'lr',\n",
    "     'main/loss', 'main/loss/loc', 'main/loss/conf',\n",
    "     'validation/main/map']),\n",
    "    trigger=log_interval)\n",
    "trainer.extend(extensions.ProgressBar(update_interval=10))\n",
    "\n",
    "trainer.extend(extensions.snapshot(), trigger=(100, 'iteration'))\n",
    "trainer.extend(\n",
    "    extensions.snapshot_object(model, 'model_iter_{.updater.iteration}'),\n",
    "    trigger=(8000, 'iteration'))\n",
    "\n",
    "if resume:\n",
    "    serializers.load_npz(resume, trainer)\n",
    "\n",
    "# 実行\n",
    "trainer.run()\n",
    "serializers.save_npz(\"model.npz\",model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論の実行\n",
    "\n",
    "demo.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ss2d.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
